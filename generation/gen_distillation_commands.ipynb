{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6981e4-4bf5-473f-bc89-6c8cab27cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "Train Set: [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 49, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 123, 125, 126, 127, 129, 130, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 150, 151, 152, 153, 156, 157, 159, 160, 162, 164, 165, 167, 168, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 245, 246, 247, 249, 251, 253, 254, 255, 256, 257, 259, 262, 263, 264, 265, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 279, 281, 283, 284, 285, 287, 288, 289, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307]\n"
     ]
    }
   ],
   "source": [
    "# First, define the training and test set\n",
    "\n",
    "import random\n",
    "\n",
    "# Total number of indices\n",
    "total_indices = 308\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(0)\n",
    "\n",
    "# Calculate the size of the test set\n",
    "test_size = int(total_indices * 0.2)\n",
    "\n",
    "# Create a list of all indices\n",
    "all_indices = list(range(total_indices))\n",
    "\n",
    "# Sample the test set indices\n",
    "test_indices = random.sample(all_indices, test_size)\n",
    "\n",
    "# Get the training set by excluding the test indices\n",
    "train_indices = [idx for idx in all_indices if idx not in test_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Test Set:\", test_indices)\n",
    "print(\"Train Set:\", train_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca3bb62-7e87-4348-becf-59e84e8cc911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Factor 0.1x:\n",
      "  Train Set (24): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 0.2x:\n",
      "  Train Set (49): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 0.3x:\n",
      "  Train Set (74): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177, 292, 139, 167, 301, 17, 176, 4, 26, 228, 125, 224, 212, 199, 1, 249, 156, 103, 79, 101, 18, 61, 182, 70, 78, 43]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 0.4x:\n",
      "  Train Set (98): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177, 292, 139, 167, 301, 17, 176, 4, 26, 228, 125, 224, 212, 199, 1, 249, 156, 103, 79, 101, 18, 61, 182, 70, 78, 43, 175, 140, 235, 23, 99, 300, 153, 30, 293, 245, 91, 36, 237, 232, 174, 246, 269, 230, 188, 90, 242, 211, 306, 209]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 0.5x:\n",
      "  Train Set (123): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177, 292, 139, 167, 301, 17, 176, 4, 26, 228, 125, 224, 212, 199, 1, 249, 156, 103, 79, 101, 18, 61, 182, 70, 78, 43, 175, 140, 235, 23, 99, 300, 153, 30, 293, 245, 91, 36, 237, 232, 174, 246, 269, 230, 188, 90, 242, 211, 306, 209, 185, 216, 201, 60, 219, 218, 10, 305, 148, 19, 25, 39, 45, 11, 210, 121, 87, 77, 288, 303, 165, 141, 80, 259, 100]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 0.6x:\n",
      "  Train Set (148): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177, 292, 139, 167, 301, 17, 176, 4, 26, 228, 125, 224, 212, 199, 1, 249, 156, 103, 79, 101, 18, 61, 182, 70, 78, 43, 175, 140, 235, 23, 99, 300, 153, 30, 293, 245, 91, 36, 237, 232, 174, 246, 269, 230, 188, 90, 242, 211, 306, 209, 185, 216, 201, 60, 219, 218, 10, 305, 148, 19, 25, 39, 45, 11, 210, 121, 87, 77, 288, 303, 165, 141, 80, 259, 100, 108, 143, 123, 58, 302, 53, 95, 16, 271, 92, 251, 54, 134, 277, 200, 3, 114, 40, 142, 275, 33, 256, 193, 24, 138]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 0.7x:\n",
      "  Train Set (172): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177, 292, 139, 167, 301, 17, 176, 4, 26, 228, 125, 224, 212, 199, 1, 249, 156, 103, 79, 101, 18, 61, 182, 70, 78, 43, 175, 140, 235, 23, 99, 300, 153, 30, 293, 245, 91, 36, 237, 232, 174, 246, 269, 230, 188, 90, 242, 211, 306, 209, 185, 216, 201, 60, 219, 218, 10, 305, 148, 19, 25, 39, 45, 11, 210, 121, 87, 77, 288, 303, 165, 141, 80, 259, 100, 108, 143, 123, 58, 302, 53, 95, 16, 271, 92, 251, 54, 134, 277, 200, 3, 114, 40, 142, 275, 33, 256, 193, 24, 138, 68, 9, 14, 263, 151, 297, 6, 191, 289, 85, 203, 106, 171, 236, 279, 127, 136, 107, 291, 206, 64, 13, 233, 129]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 0.8x:\n",
      "  Train Set (197): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177, 292, 139, 167, 301, 17, 176, 4, 26, 228, 125, 224, 212, 199, 1, 249, 156, 103, 79, 101, 18, 61, 182, 70, 78, 43, 175, 140, 235, 23, 99, 300, 153, 30, 293, 245, 91, 36, 237, 232, 174, 246, 269, 230, 188, 90, 242, 211, 306, 209, 185, 216, 201, 60, 219, 218, 10, 305, 148, 19, 25, 39, 45, 11, 210, 121, 87, 77, 288, 303, 165, 141, 80, 259, 100, 108, 143, 123, 58, 302, 53, 95, 16, 271, 92, 251, 54, 134, 277, 200, 3, 114, 40, 142, 275, 33, 256, 193, 24, 138, 68, 9, 14, 263, 151, 297, 6, 191, 289, 85, 203, 106, 171, 236, 279, 127, 136, 107, 291, 206, 64, 13, 233, 129, 5, 253, 299, 189, 267, 186, 29, 118, 190, 227, 117, 173, 184, 264, 157, 145, 22, 276, 205, 221, 198, 38, 34, 296, 273]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 0.9x:\n",
      "  Train Set (222): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177, 292, 139, 167, 301, 17, 176, 4, 26, 228, 125, 224, 212, 199, 1, 249, 156, 103, 79, 101, 18, 61, 182, 70, 78, 43, 175, 140, 235, 23, 99, 300, 153, 30, 293, 245, 91, 36, 237, 232, 174, 246, 269, 230, 188, 90, 242, 211, 306, 209, 185, 216, 201, 60, 219, 218, 10, 305, 148, 19, 25, 39, 45, 11, 210, 121, 87, 77, 288, 303, 165, 141, 80, 259, 100, 108, 143, 123, 58, 302, 53, 95, 16, 271, 92, 251, 54, 134, 277, 200, 3, 114, 40, 142, 275, 33, 256, 193, 24, 138, 68, 9, 14, 263, 151, 297, 6, 191, 289, 85, 203, 106, 171, 236, 279, 127, 136, 107, 291, 206, 64, 13, 233, 129, 5, 253, 299, 189, 267, 186, 29, 118, 190, 227, 117, 173, 184, 264, 157, 145, 22, 276, 205, 221, 198, 38, 34, 296, 273, 8, 57, 255, 274, 220, 73, 35, 214, 146, 15, 82, 76, 243, 240, 225, 307, 84, 304, 208, 234, 159, 120, 52, 116, 130]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n",
      "Scaling Factor 1.0x:\n",
      "  Train Set (247): [270, 119, 239, 285, 131, 12, 83, 162, 152, 126, 295, 247, 265, 94, 150, 110, 187, 287, 69, 160, 42, 89, 284, 238, 27, 196, 254, 81, 172, 223, 257, 192, 44, 96, 28, 231, 21, 217, 102, 147, 179, 294, 109, 137, 98, 194, 202, 66, 177, 292, 139, 167, 301, 17, 176, 4, 26, 228, 125, 224, 212, 199, 1, 249, 156, 103, 79, 101, 18, 61, 182, 70, 78, 43, 175, 140, 235, 23, 99, 300, 153, 30, 293, 245, 91, 36, 237, 232, 174, 246, 269, 230, 188, 90, 242, 211, 306, 209, 185, 216, 201, 60, 219, 218, 10, 305, 148, 19, 25, 39, 45, 11, 210, 121, 87, 77, 288, 303, 165, 141, 80, 259, 100, 108, 143, 123, 58, 302, 53, 95, 16, 271, 92, 251, 54, 134, 277, 200, 3, 114, 40, 142, 275, 33, 256, 193, 24, 138, 68, 9, 14, 263, 151, 297, 6, 191, 289, 85, 203, 106, 171, 236, 279, 127, 136, 107, 291, 206, 64, 13, 233, 129, 5, 253, 299, 189, 267, 186, 29, 118, 190, 227, 117, 173, 184, 264, 157, 145, 22, 276, 205, 221, 198, 38, 34, 296, 273, 8, 57, 255, 274, 220, 73, 35, 214, 146, 15, 82, 76, 243, 240, 225, 307, 84, 304, 208, 234, 159, 120, 52, 116, 130, 105, 168, 135, 59, 178, 164, 213, 56, 86, 65, 112, 180, 115, 88, 74, 195, 49, 281, 283, 268, 62, 93, 262, 67, 2]\n",
      "  Test Set (61): [197, 215, 20, 132, 261, 248, 207, 155, 244, 183, 298, 111, 258, 71, 144, 48, 128, 272, 75, 158, 50, 37, 169, 241, 286, 51, 181, 222, 161, 104, 282, 226, 266, 133, 31, 280, 7, 47, 204, 0, 252, 170, 124, 166, 32, 97, 290, 113, 122, 72, 278, 229, 46, 41, 163, 260, 250, 55, 154, 149, 63]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scaling factors from 0.1x to 1.0x\n",
    "scaling_factors = [i / 10 for i in range(1, 11)]\n",
    "\n",
    "# Generate scaled train and test sets\n",
    "scaled_train_test_sets = []\n",
    "for scale in scaling_factors:\n",
    "    # Scale the train and test set sizes\n",
    "    scaled_train_size = int(len(train_indices) * scale)\n",
    "    scaled_test_size = int(len(test_indices) * scale)\n",
    "    \n",
    "    # Sample scaled train and test sets deterministically\n",
    "    random.seed(0)  \n",
    "    scaled_train_indices = random.sample(train_indices, scaled_train_size)\n",
    "    # scaled_test_indices = random.sample(test_indices, scaled_test_size)\n",
    "    scaled_test_indices = test_indices  # the same test indices for all train set\n",
    "    \n",
    "    # Store the scaled train and test sets\n",
    "    scaled_train_test_sets.append((scaled_train_indices, scaled_test_indices))\n",
    "\n",
    "# Output the scaled train and test sets\n",
    "for i, (scaled_train, scaled_test) in enumerate(scaled_train_test_sets):\n",
    "    print(f\"Scaling Factor {scaling_factors[i]:.1f}x:\")\n",
    "    print(f\"  Train Set ({len(scaled_train)}): {scaled_train}\")\n",
    "    print(f\"  Test Set ({len(scaled_test)}): {scaled_test}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf5d266-fd7b-4da6-a1bb-549b37972d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Records:\n",
      "['Gendog0', 'Gendog1', 'Gendog2', 'Gendog3', 'Gendog4', 'Gendog5', 'Gendog6', 'Gendog7', 'Gendog8', 'Gendog9', 'Gendog10', 'Gendog11', 'Gendog12', 'Gendog13', 'Gendog14', 'Gendog15', 'Gendog16', 'Gendog17', 'Gendog18', 'Gendog19', 'Gendog20', 'Gendog21', 'Gendog22', 'Gendog23', 'Gendog24', 'Gendog25', 'Gendog26', 'Gendog27', 'Gendog28', 'Gendog29', 'Gendog30', 'Gendog31', 'Gendog32', 'Gendog33', 'Gendog34', 'Gendog35', 'Gendog36', 'Gendog37', 'Gendog38', 'Gendog39', 'Gendog40', 'Gendog41', 'Gendog42', 'Gendog43', 'Gendog44', 'Gendog45', 'Gendog46', 'Gendog47', 'Gendog48', 'Gendog49', 'Gendog54', 'Gendog55', 'Gendog56', 'Gendog57', 'Gendog58', 'Gendog59', 'Gendog60', 'Gendog61', 'Gendog63', 'Gendog64', 'Gendog66', 'Gendog67', 'Gendog68', 'Gendog70', 'Gendog71', 'Gendog72', 'Gendog74', 'Gendog75', 'Gendog76', 'Gendog78', 'Gendog79', 'Gendog80', 'Gendog82', 'Gendog83', 'Gendog84', 'Gendog85', 'Gendog86', 'Gendog87', 'Gendog88', 'Gendog89', 'Gendog90', 'Gendog91', 'Gendog92', 'Gendog93', 'Gendog94', 'Gendog95', 'Gendog96', 'Gendog97', 'Gendog98', 'Gendog99', 'Gendog100', 'Gendog101', 'Gendog102', 'Gendog103', 'Gendog104', 'Gendog105', 'Gendog107', 'Gendog108', 'Gendog109', 'Gendog111', 'Gendog112', 'Gendog113', 'Gendog115', 'Gendog116', 'Gendog117', 'Gendog119', 'Gendog120', 'Gendog121', 'Gendog123', 'Gendog124', 'Gendog125', 'Gendog127', 'Gendog128', 'Gendog130', 'Gendog131', 'Gendog132', 'Gendog134', 'Gendog135', 'Gendog136', 'Gendog138', 'Gendog139', 'Gendog140', 'Gendog143', 'Gendog144', 'Gendog146', 'Gendog170', 'Gendog174', 'Gendog178', 'Gendog179', 'Gendog181', 'Gendog182', 'Gendog183', 'Gendog185', 'Gendog186', 'Gendog187', 'Gendog189', 'Gendog190', 'Gendog191', 'Gendog192', 'Gendog194', 'Gendog195', 'Gendog196', 'Gendog198', 'Gendog199', 'Gendog200', 'Gendog202', 'Gendog203', 'Gendog204', 'Gendog206', 'Gendog207', 'Gendog208', 'Gendog209', 'Gendog212', 'Gendog213', 'Gendog215', 'Gendog216', 'Gendog217', 'Gendog220', 'Gendog221', 'Gendog222', 'Gendog223', 'Gendog224', 'Gendog225', 'Gendog226', 'Gendog227', 'Gendog228', 'Gendog229', 'Gendog230', 'Gendog231', 'Gendog232', 'Gendog233', 'Gendog234', 'Gendog236', 'Gendog237', 'Gendog238', 'Gendog239', 'Gendog240', 'Gendog241', 'Gendog242', 'Gendog243', 'Gendog244', 'Gendog245', 'Gendog246', 'Gendog247', 'Gendog248', 'Gendog249', 'Gendog250', 'Gendog251', 'Gendog252', 'Gendog253', 'Gendog254', 'Gendog255', 'Gendog256', 'Gendog257', 'Gendog258', 'Gendog259', 'Gendog260', 'Gendog261', 'Gendog262', 'Gendog263', 'Gendog264', 'Gendog265', 'Gendog266', 'Gendog267', 'Gendog268', 'Gendog269', 'Gendog270', 'Gendog271', 'Gendog272', 'Gendog273', 'Gendog274', 'Gendog275', 'Gendog276', 'Gendog277', 'Gendog278', 'Gendog279', 'Gendog280', 'Gendog281', 'Gendog282', 'Gendog283', 'Gendog284', 'Gendog285', 'Gendog286', 'Gendog287', 'Gendog288', 'Gendog289', 'Gendog290', 'Gendog291', 'Gendog292', 'Gendog293', 'Gendog294', 'Gendog295', 'Gendog296', 'Gendog298', 'Gendog299', 'Gendog300', 'Gendog303', 'Gendog304', 'Gendog306', 'Gendog307']\n",
      "len: 240\n",
      "\n",
      "Failed Records:\n",
      "['Gendog69', 'Gendog73', 'Gendog110', 'Gendog114', 'Gendog118', 'Gendog122', 'Gendog133', 'Gendog137', 'Gendog141', 'Gendog145', 'Gendog151', 'Gendog152', 'Gendog153', 'Gendog154', 'Gendog155', 'Gendog156', 'Gendog157', 'Gendog158', 'Gendog159', 'Gendog160', 'Gendog161', 'Gendog162', 'Gendog163', 'Gendog164', 'Gendog165', 'Gendog166', 'Gendog167', 'Gendog168', 'Gendog172', 'Gendog176', 'Gendog180', 'Gendog184', 'Gendog188', 'Gendog301', 'Gendog305']\n",
      "\n",
      "Incomplete Records:\n",
      "['Gendog65', 'Gendog77', 'Gendog81', 'Gendog106', 'Gendog126', 'Gendog129', 'Gendog142', 'Gendog147', 'Gendog148', 'Gendog149', 'Gendog150', 'Gendog171', 'Gendog175', 'Gendog297', 'Gendog302']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def check_record_status(logs_path):\n",
    "    completed_records = set()\n",
    "    completed_names = set()\n",
    "    missing_h5_record = set()\n",
    "    incomplete_records = set()\n",
    "    \n",
    "    # traverse all sub-folders in logs/rsl_rl\n",
    "    for subdir in os.listdir(logs_path):\n",
    "        subdir_path = os.path.join(logs_path, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Traverse  the latest folder\n",
    "            match = re.match(r\"(Gendog\\d+)_\", subdir)\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            if match:\n",
    "                prefix_name = match.group(1)\n",
    "                # Traverse sub-folders of all time stamps\n",
    "                time_subdirs = [d for d in os.listdir(subdir_path) if os.path.isdir(os.path.join(subdir_path, d))]\n",
    "                record_found = False\n",
    "                for time_subdir in time_subdirs:\n",
    "                    time_subdir_path = os.path.join(subdir_path, time_subdir)\n",
    "                    h5py_record_path = os.path.join(time_subdir_path, \"h5py_record\")\n",
    "                    if os.path.exists(h5py_record_path):\n",
    "                        obs_file = os.path.join(h5py_record_path, \"obs_actions_00002.h5\")\n",
    "                        if os.path.exists(obs_file):\n",
    "                            completed_records.add(prefix_name)\n",
    "                            completed_names.add(subdir)\n",
    "                            record_found = True\n",
    "                            break\n",
    "\n",
    "                if not record_found:\n",
    "                    if not any(os.path.exists(os.path.join(os.path.join(subdir_path, ts), \"h5py_record\")) for ts in time_subdirs):\n",
    "                        missing_h5_record.add(prefix_name)\n",
    "                    else:\n",
    "                        incomplete_records.add(prefix_name)\n",
    "\n",
    "    # Sort based on the results\n",
    "    def sort_by_number(prefix_list):\n",
    "        return sorted(prefix_list, key=lambda x: int(re.search(r\"\\d+\", x).group()))\n",
    "\n",
    "    return {\n",
    "        \"Completed Records\": sort_by_number(list(completed_records)),\n",
    "        \"Completed Records Names\": sort_by_number(list(completed_names)),\n",
    "        \"Missing h5py_record\": sort_by_number(list(missing_h5_record)),\n",
    "        \"Incomplete Records\": sort_by_number(list(incomplete_records))\n",
    "    }\n",
    "\n",
    "# Example\n",
    "logs_path = \"../logs/rsl_rl\"  # Modify to the actual directory\n",
    "record_status = check_record_status(logs_path)\n",
    "\n",
    "# Output the results\n",
    "print(\"Completed Records:\")\n",
    "print(record_status[\"Completed Records\"])\n",
    "print(\"len:\", len(record_status[\"Completed Records\"]))\n",
    "\n",
    "print(\"\\nFailed Records:\")\n",
    "print(record_status[\"Missing h5py_record\"])\n",
    "print(\"\\nIncomplete Records:\")\n",
    "print(record_status[\"Incomplete Records\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850df883-fac2-4638-ac95-a5c05ed5eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Factor: 0.1x\n",
      "  Unfinished Train Jobs: 0 (0/17)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 0.2x\n",
      "  Unfinished Train Jobs: 0 (0/37)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 0.3x\n",
      "  Unfinished Train Jobs: 0 (0/58)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 0.4x\n",
      "  Unfinished Train Jobs: 0 (0/77)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 0.5x\n",
      "  Unfinished Train Jobs: 0 (0/93)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 0.6x\n",
      "  Unfinished Train Jobs: 0 (0/113)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 0.7x\n",
      "  Unfinished Train Jobs: 0 (0/132)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 0.8x\n",
      "  Unfinished Train Jobs: 0 (0/151)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 0.9x\n",
      "  Unfinished Train Jobs: 0 (0/172)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n",
      "Scaling Factor: 1.0x\n",
      "  Unfinished Train Jobs: 0 (0/192)\n",
      "  Unfinished Test Jobs: 0 (0/48)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming `record_status` is already generated using the provided code\n",
    "completed_records = set(record_status[\"Completed Records\"])\n",
    "completed_folder_names = set(record_status[\"Completed Records Names\"])\n",
    "\n",
    "# Store folder names for completed and missing records\n",
    "detailed_results = []\n",
    "\n",
    "# Verbose mode: Track unavailable jobs for train and test sets\n",
    "verbose_results = []\n",
    "\n",
    "# Check for missing records in each test set\n",
    "results = []\n",
    "for i, (train_set, test_set) in enumerate(scaled_train_test_sets):\n",
    "    # Map indices to detailed folder names\n",
    "    train_set_names = [name for name in completed_folder_names if int(name.split(\"_\")[0].replace(\"Gendog\", \"\")) in train_set]\n",
    "    test_set_names = [name for name in completed_folder_names if int(name.split(\"_\")[0].replace(\"Gendog\", \"\")) in test_set]\n",
    "    \n",
    "    # Find missing and completed records for this train and test set\n",
    "    missing_train_records = set(train_set_names) - completed_folder_names\n",
    "    missing_test_records = set(test_set_names) - completed_folder_names\n",
    "    \n",
    "    # Collect folder names\n",
    "    completed_folders = [name for name in (test_set_names + train_set_names) if name in completed_folder_names]\n",
    "    missing_folders = [name for name in (test_set_names + train_set_names) if name not in completed_folder_names]\n",
    "    \n",
    "    # Store results for this scaling factor\n",
    "    results.append({\n",
    "        \"Scaling Factor\": scaling_factors[i],\n",
    "        \"Total Train Records\": len(train_set_names),\n",
    "        \"Total Test Records\": len(test_set_names),\n",
    "        \"Unfinished Train Jobs\": len(missing_train_records),\n",
    "        \"Unfinished Test Jobs\": len(missing_test_records),\n",
    "        \"Completed Records\": len(completed_folders),\n",
    "        \"Missing Records\": len(missing_folders),\n",
    "        \"Completed Folder Names\": sorted(completed_folders),\n",
    "        \"Missing Folder Names\": sorted(missing_folders),\n",
    "    })\n",
    "\n",
    "    # Store train and test sets for later use\n",
    "    detailed_results.append({\n",
    "        \"Scaling Factor\": scaling_factors[i],\n",
    "        \"Train Set Names\": train_set_names,\n",
    "        \"Test Set Names\": test_set_names,\n",
    "        \"Completed Folder Names\": completed_folders,\n",
    "        \"Missing Folder Names\": missing_folders,\n",
    "    })\n",
    "\n",
    "    # Verbose results: Track unavailable train/test jobs\n",
    "    verbose_results.append({\n",
    "        \"Scaling Factor\": scaling_factors[i],\n",
    "        \"Unfinished Train Jobs\": len(missing_train_records),\n",
    "        \"Unfinished Test Jobs\": len(missing_test_records),\n",
    "        \"Unavailable Train Ratio\": f\"{len(missing_train_records)}/{len(train_set_names)}\",\n",
    "        \"Unavailable Test Ratio\": f\"{len(missing_test_records)}/{len(test_set_names)}\",\n",
    "    })\n",
    "\n",
    "# Output results\n",
    "# for result in results:\n",
    "#     print(f\"Scaling Factor: {result['Scaling Factor']:.1f}x\")\n",
    "#     print(f\"  Total Train Records: {result['Total Train Records']}\")\n",
    "#     print(f\"  Total Test Records: {result['Total Test Records']}\")\n",
    "#     print(f\"  Completed Records: {result['Completed Records']}\")\n",
    "#     print(f\"  Missing Records: {result['Missing Records']}\")\n",
    "#     if result[\"Missing Folder Names\"]:\n",
    "#         print(f\"  Missing Folder Names: {', '.join(result['Missing Folder Names'])}\")\n",
    "#     print()\n",
    "\n",
    "# Output verbose results with train/test availability ratios\n",
    "for verbose in verbose_results:\n",
    "    print(f\"Scaling Factor: {verbose['Scaling Factor']:.1f}x\")\n",
    "    print(f\"  Unfinished Train Jobs: {verbose['Unfinished Train Jobs']} ({verbose['Unavailable Train Ratio']})\")\n",
    "    print(f\"  Unfinished Test Jobs: {verbose['Unfinished Test Jobs']} ({verbose['Unavailable Test Ratio']})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7d5487-b315-4256-8e91-f94e16ff70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0fa153-34b8-424c-b621-5e8c0c796757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 job YAML files in 'jobs_scaling_factors'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configuration\n",
    "output_folder = \"jobs_scaling_factors\"  # Folder to store YAML files\n",
    "logs_path = \"../logs/rsl_rl\"  # Logs directory path for status check\n",
    "yaml_template = \"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  namespace: ucsd-haosulab\n",
    "spec:\n",
    "  ttlSecondsAfterFinished: 604800\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        nautilus.io/rl: \"true\"\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: gpu-container\n",
    "          image: albert01102/cuda12.4.1_ubuntu22.04_embodiment:isaac-v1.1-nodisplay\n",
    "          command:\n",
    "            - \"/bin/bash\"\n",
    "            - \"-c\"\n",
    "          args:\n",
    "            - |\n",
    "              cd /bai-fast-vol/code/embodiment-scaling-law && {command}\n",
    "          resources:\n",
    "            requests:\n",
    "              cpu: \"16\"\n",
    "              memory: \"128Gi\"\n",
    "              nvidia.com/gpu: \"1\"\n",
    "            limits:\n",
    "              cpu: \"32\"\n",
    "              memory: \"128Gi\"\n",
    "              nvidia.com/gpu: \"1\"\n",
    "          volumeMounts:\n",
    "            - name: dshm\n",
    "              mountPath: /dev/shm\n",
    "            - name: bai-fast-vol\n",
    "              mountPath: /bai-fast-vol\n",
    "      volumes:\n",
    "        - name: dshm\n",
    "          emptyDir:\n",
    "            medium: Memory\n",
    "        - name: bai-fast-vol\n",
    "          persistentVolumeClaim:\n",
    "            claimName: bai-fast-vol\n",
    "      restartPolicy: Never\n",
    "      affinity:\n",
    "        nodeAffinity:\n",
    "          requiredDuringSchedulingIgnoredDuringExecution:\n",
    "            nodeSelectorTerms:\n",
    "              - matchExpressions:\n",
    "                  - key: nvidia.com/gpu.product\n",
    "                    operator: In\n",
    "                    values:\n",
    "                      - NVIDIA-GeForce-RTX-4090\n",
    "                      - NVIDIA-GeForce-RTX-3090\n",
    "                      - NVIDIA-A100-80GB-PCIe-MIG-1g.10gb   # threaded multi-instance GPU\n",
    "                      - NVIDIA-A100-PCIE-40GB\n",
    "                      - NVIDIA-A100-80GB-PCIe\n",
    "                      - NVIDIA-A100-SXM4-80GB\n",
    "                      - NVIDIA-RTX-A6000  # 10% weaker than RTX 3090\n",
    "                      - NVIDIA-A40    # 20% weaker than RTX 3090\n",
    "  backoffLimit: 0\n",
    "\"\"\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Generate YAML files for each scaling factor\n",
    "for i, result in enumerate(detailed_results):  # Use detailed_results instead of scaled_train_test_sets\n",
    "    train_set = result[\"Train Set Names\"]\n",
    "    test_set = result[\"Test Set Names\"]\n",
    "    # print(train_set)\n",
    "    \n",
    "    # Use the long detailed folder names for train and test sets\n",
    "    train_set_str = \" \".join(train_set)\n",
    "    test_set_str = \" \".join(test_set)\n",
    "    \n",
    "    # Command to execute training for this scaling factor\n",
    "    command = (\n",
    "        f\"/workspace/isaaclab/isaaclab.sh -p scripts/rsl_rl/run_distillation.py \"\n",
    "        f\"--train_set {train_set_str} \"\n",
    "        f\"--test_set {test_set_str} \"\n",
    "        f\"--model urma \"\n",
    "        f\"--exp_name scaling_factor_{result['Scaling Factor']:.1f} \"\n",
    "        f\"--batch_size 512 \"\n",
    "        f\"--lr 3e-4 \"\n",
    "        f\"--num_workers 24 \"\n",
    "        f\"--num_epochs 50 \"\n",
    "        f\"--gradient_acc_steps 2\"\n",
    "    )\n",
    "    \n",
    "    # Job name\n",
    "    job_name = f\"bai-distillation-scaling-{i+1}\"\n",
    "    \n",
    "    # Generate YAML content\n",
    "    yaml_content = yaml_template.format(job_name=job_name, command=command)\n",
    "    \n",
    "    # Write to YAML file\n",
    "    yaml_file = os.path.join(output_folder, f\"{job_name}.yaml\")\n",
    "    with open(yaml_file, \"w\") as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "print(f\"Generated {len(detailed_results)} job YAML files in '{output_folder}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2244a09-5ce1-4969-8231-9bac9d17a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission script: jobs_scaling_factors/submit_all_jobs.sh\n",
      "Deletion script: jobs_scaling_factors/delete_all_jobs.sh\n"
     ]
    }
   ],
   "source": [
    "# Paths for submission and deletion scripts\n",
    "submission_script = os.path.join(output_folder, \"submit_all_jobs.sh\")\n",
    "deletion_script = os.path.join(output_folder, \"delete_all_jobs.sh\")\n",
    "\n",
    "# Get all job files generated\n",
    "job_files = [f for f in os.listdir(output_folder) if f.endswith(\".yaml\")]\n",
    "\n",
    "# Generate submission script\n",
    "with open(submission_script, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\\n\")\n",
    "    for job_file in job_files:\n",
    "        f.write(f\"kubectl create -f {os.path.join(output_folder, job_file)}\\n\")\n",
    "\n",
    "# Make the submission script executable\n",
    "os.chmod(submission_script, 0o755)\n",
    "\n",
    "# Generate deletion script\n",
    "with open(deletion_script, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\\n\")\n",
    "    for job_file in job_files:\n",
    "        job_name = job_file.replace(\".yaml\", \"\")  # Extract job name from the file name\n",
    "        f.write(f\"kubectl delete job {job_name}\\n\")\n",
    "\n",
    "# Make the deletion script executable\n",
    "os.chmod(deletion_script, 0o755)\n",
    "\n",
    "print(f\"Submission script: {submission_script}\")\n",
    "print(f\"Deletion script: {deletion_script}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ec6285-e0c5-4b1a-a53c-0b8b763019f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_scaling_factors/\n",
      "jobs_scaling_factors/bai-distillation-scaling-6.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-9.yaml\n",
      "jobs_scaling_factors/submit_all_jobs.sh\n",
      "jobs_scaling_factors/bai-distillation-scaling-4.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-2.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-10.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-1.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-7.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-8.yaml\n",
      "jobs_scaling_factors/.ipynb_checkpoints/\n",
      "jobs_scaling_factors/.ipynb_checkpoints/bai-distillation-scaling-10-checkpoint.yaml\n",
      "jobs_scaling_factors/.ipynb_checkpoints/gen_distillation_commands-checkpoint.ipynb\n",
      "jobs_scaling_factors/.ipynb_checkpoints/submit_all_jobs-checkpoint.sh\n",
      "jobs_scaling_factors/gen_distillation_commands.ipynb\n",
      "jobs_scaling_factors/bai-distillation-scaling-5.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-3.yaml\n",
      "jobs_scaling_factors/delete_all_jobs.sh\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf jobs_scaling_factors.tar jobs_scaling_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489057a5-7d85-45ba-815a-8ed9c3621c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
