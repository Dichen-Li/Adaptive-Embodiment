{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6981e4-4bf5-473f-bc89-6c8cab27cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set: [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "Train Set: [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 49, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 123, 125, 126, 127, 129, 130, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 150, 151, 152, 153, 156, 157, 159, 160, 162, 164, 165, 167, 168, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 245, 246, 247, 249, 251, 253, 254, 255, 256, 257, 259, 262, 263, 264, 265, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 279, 281, 283, 284, 285, 287, 288, 289, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 314, 315, 317, 318, 319, 321, 322, 323, 324, 325, 326, 328, 329]\n"
     ]
    }
   ],
   "source": [
    "# First, define the training and test set\n",
    "\n",
    "import random\n",
    "\n",
    "# Total number of indices\n",
    "total_indices = 330\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(0)\n",
    "\n",
    "GT_index = 15\n",
    "\n",
    "# Calculate the size of the test set\n",
    "test_size = int(total_indices * 0.2)\n",
    "\n",
    "# Create a list of all indices\n",
    "all_indices = list(range(total_indices))\n",
    "\n",
    "# Sample the test set indices\n",
    "test_indices = random.sample(all_indices, test_size)\n",
    "\n",
    "if not GT_index in test_indices:\n",
    "    test_indices.append(GT_index)\n",
    "    \n",
    "# Get the training set by excluding the test indices\n",
    "train_indices = [idx for idx in all_indices if idx not in test_indices]\n",
    "\n",
    "train_indices.sort()\n",
    "test_indices.sort()\n",
    "\n",
    "# Print the results\n",
    "print(\"Test Set:\", test_indices)\n",
    "print(\"Train Set:\", train_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca3bb62-7e87-4348-becf-59e84e8cc911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Factor 0.1x:\n",
      "  Train Set (26): [24, 28, 43, 69, 81, 89, 110, 160, 165, 172, 187, 193, 196, 223, 238, 243, 254, 257, 269, 287, 292, 305, 310, 315, 323, 324]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 0.2x:\n",
      "  Train Set (52): [18, 22, 24, 28, 29, 43, 45, 66, 69, 81, 83, 89, 96, 98, 102, 109, 110, 137, 139, 147, 150, 160, 165, 167, 172, 177, 179, 187, 192, 193, 194, 196, 202, 217, 223, 231, 238, 243, 254, 256, 257, 269, 271, 287, 289, 292, 301, 305, 310, 315, 323, 324]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 0.3x:\n",
      "  Train Set (78): [1, 4, 18, 19, 22, 24, 27, 28, 29, 43, 44, 45, 62, 66, 69, 70, 78, 79, 81, 83, 89, 96, 98, 101, 102, 103, 109, 110, 125, 137, 139, 140, 147, 150, 156, 160, 165, 167, 172, 175, 176, 177, 179, 182, 187, 192, 193, 194, 196, 199, 202, 212, 217, 223, 224, 228, 231, 238, 243, 247, 254, 256, 257, 259, 269, 271, 275, 287, 289, 291, 292, 299, 301, 305, 310, 315, 323, 324]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 0.4x:\n",
      "  Train Set (105): [1, 4, 18, 19, 22, 24, 27, 28, 29, 33, 38, 43, 44, 45, 61, 62, 66, 69, 70, 78, 79, 81, 83, 89, 90, 91, 94, 96, 98, 99, 101, 102, 103, 109, 110, 119, 125, 137, 139, 140, 147, 150, 153, 156, 160, 162, 165, 167, 172, 174, 175, 176, 177, 179, 182, 185, 187, 188, 190, 192, 193, 194, 196, 199, 202, 212, 217, 219, 223, 224, 227, 228, 230, 231, 234, 237, 238, 243, 245, 247, 254, 256, 257, 259, 263, 269, 271, 273, 275, 287, 288, 289, 291, 292, 296, 299, 301, 305, 310, 315, 317, 323, 324, 326, 328]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 0.5x:\n",
      "  Train Set (131): [1, 4, 10, 11, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 33, 35, 38, 40, 43, 44, 45, 49, 61, 62, 66, 69, 70, 77, 78, 79, 81, 83, 87, 89, 90, 91, 94, 96, 98, 99, 101, 102, 103, 109, 110, 119, 121, 125, 131, 137, 139, 140, 141, 147, 148, 150, 152, 153, 156, 160, 162, 165, 167, 168, 172, 174, 175, 176, 177, 179, 180, 182, 185, 187, 188, 190, 192, 193, 194, 196, 198, 199, 202, 212, 214, 217, 219, 223, 224, 227, 228, 230, 231, 234, 237, 238, 240, 243, 245, 246, 247, 254, 256, 257, 259, 263, 267, 268, 269, 271, 273, 275, 287, 288, 289, 291, 292, 296, 297, 299, 301, 305, 310, 314, 315, 317, 319, 323, 324, 326, 328]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 0.6x:\n",
      "  Train Set (157): [1, 4, 5, 6, 9, 10, 11, 14, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 33, 34, 35, 38, 40, 43, 44, 45, 49, 56, 61, 62, 66, 68, 69, 70, 77, 78, 79, 81, 83, 85, 86, 87, 89, 90, 91, 94, 96, 98, 99, 101, 102, 103, 108, 109, 110, 119, 121, 123, 125, 127, 131, 135, 137, 139, 140, 141, 145, 147, 148, 150, 152, 153, 156, 160, 162, 165, 167, 168, 171, 172, 174, 175, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 192, 193, 194, 195, 196, 198, 199, 202, 209, 212, 214, 216, 217, 219, 223, 224, 227, 228, 230, 231, 234, 237, 238, 240, 243, 245, 246, 247, 254, 256, 257, 259, 263, 267, 268, 269, 271, 273, 274, 275, 279, 287, 288, 289, 291, 292, 295, 296, 297, 299, 301, 304, 305, 310, 314, 315, 317, 319, 323, 324, 325, 326, 328, 329]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 0.7x:\n",
      "  Train Set (184): [1, 3, 4, 5, 6, 9, 10, 11, 13, 14, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 38, 40, 43, 44, 45, 49, 56, 61, 62, 64, 66, 68, 69, 70, 77, 78, 79, 81, 83, 85, 86, 87, 89, 90, 91, 94, 96, 98, 99, 101, 102, 103, 106, 108, 109, 110, 117, 119, 120, 121, 123, 125, 127, 129, 131, 134, 135, 137, 138, 139, 140, 141, 143, 145, 146, 147, 148, 150, 151, 152, 153, 156, 157, 159, 160, 162, 165, 167, 168, 171, 172, 173, 174, 175, 176, 177, 179, 180, 182, 184, 185, 187, 188, 190, 192, 193, 194, 195, 196, 198, 199, 200, 202, 203, 206, 209, 211, 212, 214, 216, 217, 219, 221, 223, 224, 227, 228, 230, 231, 234, 237, 238, 240, 243, 245, 246, 247, 254, 256, 257, 259, 262, 263, 265, 267, 268, 269, 270, 271, 273, 274, 275, 279, 287, 288, 289, 291, 292, 293, 295, 296, 297, 299, 301, 304, 305, 310, 314, 315, 317, 318, 319, 321, 323, 324, 325, 326, 328, 329]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 0.8x:\n",
      "  Train Set (210): [1, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 38, 40, 43, 44, 45, 49, 56, 57, 58, 60, 61, 62, 64, 66, 67, 68, 69, 70, 73, 76, 77, 78, 79, 81, 83, 85, 86, 87, 89, 90, 91, 94, 95, 96, 98, 99, 101, 102, 103, 106, 108, 109, 110, 116, 117, 119, 120, 121, 123, 125, 126, 127, 129, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 150, 151, 152, 153, 156, 157, 159, 160, 162, 165, 167, 168, 171, 172, 173, 174, 175, 176, 177, 179, 180, 182, 184, 185, 187, 188, 189, 190, 192, 193, 194, 195, 196, 198, 199, 200, 202, 203, 205, 206, 208, 209, 210, 211, 212, 214, 216, 217, 219, 220, 221, 223, 224, 227, 228, 230, 231, 233, 234, 236, 237, 238, 240, 243, 245, 246, 247, 254, 256, 257, 259, 262, 263, 265, 267, 268, 269, 270, 271, 273, 274, 275, 277, 279, 281, 287, 288, 289, 291, 292, 293, 295, 296, 297, 299, 301, 302, 303, 304, 305, 306, 309, 310, 314, 315, 317, 318, 319, 321, 323, 324, 325, 326, 328, 329]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 0.9x:\n",
      "  Train Set (236): [1, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 38, 39, 40, 43, 44, 45, 49, 52, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 73, 74, 76, 77, 78, 79, 81, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 99, 101, 102, 103, 106, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 123, 125, 126, 127, 129, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 150, 151, 152, 153, 156, 157, 159, 160, 162, 164, 165, 167, 168, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 205, 206, 208, 209, 210, 211, 212, 214, 216, 217, 219, 220, 221, 223, 224, 225, 227, 228, 230, 231, 233, 234, 235, 236, 237, 238, 240, 243, 245, 246, 247, 249, 253, 254, 255, 256, 257, 259, 262, 263, 264, 265, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 279, 281, 285, 287, 288, 289, 291, 292, 293, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 309, 310, 311, 314, 315, 317, 318, 319, 321, 323, 324, 325, 326, 328, 329]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n",
      "Scaling Factor 1.0x:\n",
      "  Train Set (263): [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 49, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 123, 125, 126, 127, 129, 130, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 150, 151, 152, 153, 156, 157, 159, 160, 162, 164, 165, 167, 168, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213, 214, 216, 217, 218, 219, 220, 221, 223, 224, 225, 227, 228, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 243, 245, 246, 247, 249, 251, 253, 254, 255, 256, 257, 259, 262, 263, 264, 265, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 279, 281, 283, 284, 285, 287, 288, 289, 291, 292, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 314, 315, 317, 318, 319, 321, 322, 323, 324, 325, 326, 328, 329]\n",
      "  Test Set (67): [0, 7, 15, 20, 31, 32, 37, 41, 46, 47, 48, 50, 51, 55, 71, 72, 75, 97, 104, 111, 113, 122, 124, 128, 132, 133, 144, 149, 154, 155, 158, 161, 163, 166, 169, 170, 181, 183, 197, 204, 207, 215, 222, 226, 229, 241, 244, 248, 250, 252, 258, 260, 261, 266, 272, 278, 280, 282, 286, 290, 298, 308, 312, 313, 316, 320, 327]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scaling factors from 0.1x to 1.0x\n",
    "scaling_factors = [i / 10 for i in range(1, 11)]\n",
    "\n",
    "# Generate scaled train and test sets\n",
    "scaled_train_test_sets = []\n",
    "for scale in scaling_factors:\n",
    "    # Scale the train and test set sizes\n",
    "    scaled_train_size = int(len(train_indices) * scale)\n",
    "    scaled_test_size = int(len(test_indices) * scale)\n",
    "    \n",
    "    # Sample scaled train and test sets deterministically\n",
    "    random.seed(0)  \n",
    "    scaled_train_indices = random.sample(train_indices, scaled_train_size)\n",
    "    # scaled_test_indices = random.sample(test_indices, scaled_test_size)\n",
    "    scaled_test_indices = test_indices  # the same test indices for all train set\n",
    "\n",
    "    scaled_train_indices.sort()\n",
    "    scaled_test_indices.sort()\n",
    "    \n",
    "    # Store the scaled train and test sets\n",
    "    scaled_train_test_sets.append((scaled_train_indices, scaled_test_indices))\n",
    "\n",
    "# Output the scaled train and test sets\n",
    "for i, (scaled_train, scaled_test) in enumerate(scaled_train_test_sets):\n",
    "    print(f\"Scaling Factor {scaling_factors[i]:.1f}x:\")\n",
    "    print(f\"  Train Set ({len(scaled_train)}): {scaled_train}\")\n",
    "    print(f\"  Test Set ({len(scaled_test)}): {scaled_test}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf5d266-fd7b-4da6-a1bb-549b37972d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Records:\n",
      "['Genhumanoid1', 'Genhumanoid2', 'Genhumanoid3', 'Genhumanoid4', 'Genhumanoid5', 'Genhumanoid6', 'Genhumanoid7', 'Genhumanoid8', 'Genhumanoid9', 'Genhumanoid10', 'Genhumanoid11', 'Genhumanoid12', 'Genhumanoid13', 'Genhumanoid14', 'Genhumanoid15', 'Genhumanoid16', 'Genhumanoid17', 'Genhumanoid18', 'Genhumanoid19', 'Genhumanoid20', 'Genhumanoid21', 'Genhumanoid22', 'Genhumanoid23', 'Genhumanoid24', 'Genhumanoid25', 'Genhumanoid26', 'Genhumanoid27', 'Genhumanoid28', 'Genhumanoid29', 'Genhumanoid30', 'Genhumanoid31', 'Genhumanoid32', 'Genhumanoid33', 'Genhumanoid34', 'Genhumanoid35', 'Genhumanoid36', 'Genhumanoid37', 'Genhumanoid38', 'Genhumanoid39', 'Genhumanoid40', 'Genhumanoid41', 'Genhumanoid42', 'Genhumanoid43', 'Genhumanoid44', 'Genhumanoid45', 'Genhumanoid46', 'Genhumanoid47', 'Genhumanoid48', 'Genhumanoid49', 'Genhumanoid50', 'Genhumanoid51', 'Genhumanoid52', 'Genhumanoid53', 'Genhumanoid54', 'Genhumanoid55', 'Genhumanoid56', 'Genhumanoid57', 'Genhumanoid58', 'Genhumanoid59', 'Genhumanoid60', 'Genhumanoid61', 'Genhumanoid62', 'Genhumanoid63', 'Genhumanoid64', 'Genhumanoid65', 'Genhumanoid66', 'Genhumanoid67', 'Genhumanoid68', 'Genhumanoid69', 'Genhumanoid70', 'Genhumanoid71', 'Genhumanoid72', 'Genhumanoid73', 'Genhumanoid75', 'Genhumanoid76', 'Genhumanoid77', 'Genhumanoid78', 'Genhumanoid79', 'Genhumanoid80', 'Genhumanoid81', 'Genhumanoid82', 'Genhumanoid83', 'Genhumanoid84', 'Genhumanoid85', 'Genhumanoid86', 'Genhumanoid87', 'Genhumanoid88', 'Genhumanoid89', 'Genhumanoid90', 'Genhumanoid91', 'Genhumanoid92', 'Genhumanoid93', 'Genhumanoid94', 'Genhumanoid95', 'Genhumanoid96', 'Genhumanoid97', 'Genhumanoid98', 'Genhumanoid99', 'Genhumanoid100', 'Genhumanoid101', 'Genhumanoid102', 'Genhumanoid103', 'Genhumanoid104', 'Genhumanoid105', 'Genhumanoid106', 'Genhumanoid107', 'Genhumanoid108', 'Genhumanoid109', 'Genhumanoid110', 'Genhumanoid111', 'Genhumanoid112', 'Genhumanoid113', 'Genhumanoid114', 'Genhumanoid115', 'Genhumanoid116', 'Genhumanoid117', 'Genhumanoid118', 'Genhumanoid119', 'Genhumanoid120', 'Genhumanoid121', 'Genhumanoid122', 'Genhumanoid123', 'Genhumanoid124', 'Genhumanoid125', 'Genhumanoid126', 'Genhumanoid127', 'Genhumanoid128', 'Genhumanoid129', 'Genhumanoid130', 'Genhumanoid131', 'Genhumanoid132', 'Genhumanoid133', 'Genhumanoid134', 'Genhumanoid135', 'Genhumanoid136', 'Genhumanoid137', 'Genhumanoid138', 'Genhumanoid139', 'Genhumanoid140', 'Genhumanoid141', 'Genhumanoid142', 'Genhumanoid143', 'Genhumanoid144', 'Genhumanoid145', 'Genhumanoid146', 'Genhumanoid147', 'Genhumanoid148', 'Genhumanoid149', 'Genhumanoid150', 'Genhumanoid151', 'Genhumanoid152', 'Genhumanoid153', 'Genhumanoid154', 'Genhumanoid155', 'Genhumanoid156', 'Genhumanoid157', 'Genhumanoid158', 'Genhumanoid159', 'Genhumanoid160', 'Genhumanoid161', 'Genhumanoid162', 'Genhumanoid163', 'Genhumanoid164', 'Genhumanoid165', 'Genhumanoid166', 'Genhumanoid167', 'Genhumanoid168', 'Genhumanoid169', 'Genhumanoid170', 'Genhumanoid171', 'Genhumanoid172', 'Genhumanoid173', 'Genhumanoid174', 'Genhumanoid175', 'Genhumanoid176', 'Genhumanoid177', 'Genhumanoid178', 'Genhumanoid179', 'Genhumanoid181', 'Genhumanoid182', 'Genhumanoid183', 'Genhumanoid184', 'Genhumanoid185', 'Genhumanoid186', 'Genhumanoid187', 'Genhumanoid188', 'Genhumanoid189', 'Genhumanoid190', 'Genhumanoid191', 'Genhumanoid192', 'Genhumanoid193', 'Genhumanoid194', 'Genhumanoid195', 'Genhumanoid196', 'Genhumanoid197', 'Genhumanoid198', 'Genhumanoid199', 'Genhumanoid201', 'Genhumanoid203', 'Genhumanoid204', 'Genhumanoid205', 'Genhumanoid206', 'Genhumanoid207', 'Genhumanoid209', 'Genhumanoid211', 'Genhumanoid212', 'Genhumanoid213', 'Genhumanoid214', 'Genhumanoid215', 'Genhumanoid216', 'Genhumanoid218', 'Genhumanoid219', 'Genhumanoid222', 'Genhumanoid223', 'Genhumanoid226', 'Genhumanoid227', 'Genhumanoid228', 'Genhumanoid230', 'Genhumanoid231', 'Genhumanoid232', 'Genhumanoid234', 'Genhumanoid235', 'Genhumanoid236', 'Genhumanoid238', 'Genhumanoid239', 'Genhumanoid240', 'Genhumanoid241', 'Genhumanoid242', 'Genhumanoid243', 'Genhumanoid244', 'Genhumanoid245', 'Genhumanoid246', 'Genhumanoid247', 'Genhumanoid248', 'Genhumanoid249', 'Genhumanoid250', 'Genhumanoid251', 'Genhumanoid252', 'Genhumanoid253', 'Genhumanoid254', 'Genhumanoid255', 'Genhumanoid256', 'Genhumanoid257', 'Genhumanoid258', 'Genhumanoid259', 'Genhumanoid266', 'Genhumanoid267', 'Genhumanoid269', 'Genhumanoid270', 'Genhumanoid271', 'Genhumanoid273', 'Genhumanoid274', 'Genhumanoid275', 'Genhumanoid277', 'Genhumanoid278', 'Genhumanoid279', 'Genhumanoid281', 'Genhumanoid282', 'Genhumanoid283', 'Genhumanoid285', 'Genhumanoid286', 'Genhumanoid287', 'Genhumanoid288', 'Genhumanoid289', 'Genhumanoid290', 'Genhumanoid291', 'Genhumanoid292', 'Genhumanoid293', 'Genhumanoid294', 'Genhumanoid295', 'Genhumanoid296', 'Genhumanoid297', 'Genhumanoid298', 'Genhumanoid299', 'Genhumanoid300', 'Genhumanoid301', 'Genhumanoid302', 'Genhumanoid303', 'Genhumanoid304', 'Genhumanoid305', 'Genhumanoid306', 'Genhumanoid307', 'Genhumanoid308', 'Genhumanoid309', 'Genhumanoid310', 'Genhumanoid311', 'Genhumanoid312', 'Genhumanoid314', 'Genhumanoid315', 'Genhumanoid316', 'Genhumanoid318', 'Genhumanoid319', 'Genhumanoid320', 'Genhumanoid321', 'Genhumanoid322', 'Genhumanoid323', 'Genhumanoid324', 'Genhumanoid326', 'Genhumanoid327', 'Genhumanoid328', 'Genhumanoid329']\n",
      "len: 301\n",
      "\n",
      "Failed Records:\n",
      "['Genhumanoid0', 'Genhumanoid74', 'Genhumanoid180', 'Genhumanoid210', 'Genhumanoid220', 'Genhumanoid224', 'Genhumanoid265', 'Genhumanoid313', 'Genhumanoid317', 'Genhumanoid325']\n",
      "\n",
      "Incomplete Records:\n",
      "['Genhumanoid200', 'Genhumanoid202', 'Genhumanoid208']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def check_record_status(logs_path):\n",
    "    completed_records = set()\n",
    "    completed_names = set()\n",
    "    missing_h5_record = set()\n",
    "    incomplete_records = set()\n",
    "\n",
    "    # Traverse all sub-folders in logs/rsl_rl\n",
    "    for subdir in os.listdir(logs_path):\n",
    "        subdir_path = os.path.join(logs_path, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            # Match prefix name\n",
    "            match = re.match(r\"(Genhumanoid\\d+)_\", subdir)\n",
    "            if match:\n",
    "                prefix_name = match.group(1)\n",
    "                # Get all timestamp sub-folders\n",
    "                time_subdirs = [\n",
    "                    d for d in os.listdir(subdir_path)\n",
    "                    if os.path.isdir(os.path.join(subdir_path, d))\n",
    "                ]\n",
    "                if time_subdirs:\n",
    "                    # Sort to find the latest folder\n",
    "                    latest_subdir = sorted(time_subdirs)[-1]\n",
    "                    latest_subdir_path = os.path.join(subdir_path, latest_subdir)\n",
    "                    h5py_record_path = os.path.join(latest_subdir_path, \"h5py_record\")\n",
    "                    if os.path.exists(h5py_record_path):\n",
    "                        obs_file = os.path.join(h5py_record_path, \"obs_actions_00004.h5\")\n",
    "                        if os.path.exists(obs_file):\n",
    "                            completed_records.add(prefix_name)\n",
    "                            completed_names.add(subdir)\n",
    "                        else:\n",
    "                            incomplete_records.add(prefix_name)\n",
    "                    else:\n",
    "                        missing_h5_record.add(prefix_name)\n",
    "\n",
    "    # Sort based on the results\n",
    "    def sort_by_number(prefix_list):\n",
    "        return sorted(prefix_list, key=lambda x: int(re.search(r\"\\d+\", x).group()))\n",
    "\n",
    "    return {\n",
    "        \"Completed Records\": sort_by_number(list(completed_records)),\n",
    "        \"Completed Records Names\": sort_by_number(list(completed_names)),\n",
    "        \"Missing h5py_record\": sort_by_number(list(missing_h5_record)),\n",
    "        \"Incomplete Records\": sort_by_number(list(incomplete_records))\n",
    "    }\n",
    "\n",
    "# Example\n",
    "logs_path = \"/home/liudai/hdd_0/projects/cross_em/data/logs_all_v0/genhumanoid/logs/rsl_rl\" # Modify to the actual directory\n",
    "record_status = check_record_status(logs_path)\n",
    "\n",
    "# Output the results\n",
    "print(\"Completed Records:\")\n",
    "print(record_status[\"Completed Records\"])\n",
    "print(\"len:\", len(record_status[\"Completed Records\"]))\n",
    "\n",
    "print(\"\\nFailed Records:\")\n",
    "print(record_status[\"Missing h5py_record\"])\n",
    "print(\"\\nIncomplete Records:\")\n",
    "print(record_status[\"Incomplete Records\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850df883-fac2-4638-ac95-a5c05ed5eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Factor: 0.1x\n",
      "  Unfinished Train Jobs: 0 (0/26)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 0.2x\n",
      "  Unfinished Train Jobs: 0 (0/50)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 0.3x\n",
      "  Unfinished Train Jobs: 0 (0/75)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 0.4x\n",
      "  Unfinished Train Jobs: 0 (0/99)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 0.5x\n",
      "  Unfinished Train Jobs: 0 (0/123)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 0.6x\n",
      "  Unfinished Train Jobs: 0 (0/148)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 0.7x\n",
      "  Unfinished Train Jobs: 0 (0/171)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 0.8x\n",
      "  Unfinished Train Jobs: 0 (0/193)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 0.9x\n",
      "  Unfinished Train Jobs: 0 (0/215)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n",
      "Scaling Factor: 1.0x\n",
      "  Unfinished Train Jobs: 0 (0/241)\n",
      "  Unfinished Test Jobs: 0 (0/60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming `record_status` is already generated using the provided code\n",
    "completed_records = set(record_status[\"Completed Records\"])\n",
    "completed_folder_names = set(record_status[\"Completed Records Names\"])\n",
    "\n",
    "# Store folder names for completed and missing records\n",
    "detailed_results = []\n",
    "\n",
    "# Verbose mode: Track unavailable jobs for train and test sets\n",
    "verbose_results = []\n",
    "\n",
    "# Check for missing records in each test set\n",
    "results = []\n",
    "for i, (train_set, test_set) in enumerate(scaled_train_test_sets):\n",
    "    # Map indices to detailed folder names\n",
    "    train_set_names = [name for name in completed_folder_names if int(name.split(\"_\")[0].replace(\"Genhumanoid\", \"\")) in train_set]\n",
    "    test_set_names = [name for name in completed_folder_names if int(name.split(\"_\")[0].replace(\"Genhumanoid\", \"\")) in test_set]\n",
    "\n",
    "    train_set_names = sorted(\n",
    "        train_set_names,\n",
    "        key=lambda x: int(x.split(\"_\")[0].replace(\"Genhumanoid\", \"\"))\n",
    "    )\n",
    "    test_set_names = sorted(\n",
    "        test_set_names,\n",
    "        key=lambda x: int(x.split(\"_\")[0].replace(\"Genhumanoid\", \"\"))\n",
    "    )\n",
    "    \n",
    "    # Find missing and completed records for this train and test set\n",
    "    missing_train_records = set(train_set_names) - completed_folder_names\n",
    "    missing_test_records = set(test_set_names) - completed_folder_names\n",
    "    \n",
    "    # Collect folder names\n",
    "    completed_folders = [name for name in (test_set_names + train_set_names) if name in completed_folder_names]\n",
    "    missing_folders = [name for name in (test_set_names + train_set_names) if name not in completed_folder_names]\n",
    "    \n",
    "    # Store results for this scaling factor\n",
    "    results.append({\n",
    "        \"Scaling Factor\": scaling_factors[i],\n",
    "        \"Total Train Records\": len(train_set_names),\n",
    "        \"Total Test Records\": len(test_set_names),\n",
    "        \"Unfinished Train Jobs\": len(missing_train_records),\n",
    "        \"Unfinished Test Jobs\": len(missing_test_records),\n",
    "        \"Completed Records\": len(completed_folders),\n",
    "        \"Missing Records\": len(missing_folders),\n",
    "        \"Completed Folder Names\": sorted(completed_folders),\n",
    "        \"Missing Folder Names\": sorted(missing_folders),\n",
    "    })\n",
    "\n",
    "    # Store train and test sets for later use\n",
    "    detailed_results.append({\n",
    "        \"Scaling Factor\": scaling_factors[i],\n",
    "        \"Train Set Names\": train_set_names,\n",
    "        \"Test Set Names\": test_set_names,\n",
    "        \"Completed Folder Names\": completed_folders,\n",
    "        \"Missing Folder Names\": missing_folders,\n",
    "    })\n",
    "\n",
    "    # Verbose results: Track unavailable train/test jobs\n",
    "    verbose_results.append({\n",
    "        \"Scaling Factor\": scaling_factors[i],\n",
    "        \"Unfinished Train Jobs\": len(missing_train_records),\n",
    "        \"Unfinished Test Jobs\": len(missing_test_records),\n",
    "        \"Unavailable Train Ratio\": f\"{len(missing_train_records)}/{len(train_set_names)}\",\n",
    "        \"Unavailable Test Ratio\": f\"{len(missing_test_records)}/{len(test_set_names)}\",\n",
    "    })\n",
    "\n",
    "# Output results\n",
    "# for result in results:\n",
    "#     print(f\"Scaling Factor: {result['Scaling Factor']:.1f}x\")\n",
    "#     print(f\"  Total Train Records: {result['Total Train Records']}\")\n",
    "#     print(f\"  Total Test Records: {result['Total Test Records']}\")\n",
    "#     print(f\"  Completed Records: {result['Completed Records']}\")\n",
    "#     print(f\"  Missing Records: {result['Missing Records']}\")\n",
    "#     if result[\"Missing Folder Names\"]:\n",
    "#         print(f\"  Missing Folder Names: {', '.join(result['Missing Folder Names'])}\")\n",
    "#     print()\n",
    "\n",
    "# Output verbose results with train/test availability ratios\n",
    "for verbose in verbose_results:\n",
    "    print(f\"Scaling Factor: {verbose['Scaling Factor']:.1f}x\")\n",
    "    print(f\"  Unfinished Train Jobs: {verbose['Unfinished Train Jobs']} ({verbose['Unavailable Train Ratio']})\")\n",
    "    print(f\"  Unfinished Test Jobs: {verbose['Unfinished Test Jobs']} ({verbose['Unavailable Test Ratio']})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7d5487-b315-4256-8e91-f94e16ff70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0fa153-34b8-424c-b621-5e8c0c796757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 job YAML files in 'jobs_scaling_factors'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configuration\n",
    "output_folder = \"jobs_scaling_factors\"  # Folder to store YAML files\n",
    "logs_path = \"../logs/rsl_rl\"  # Logs directory path for status check\n",
    "yaml_template = \"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: {job_name}\n",
    "  namespace: ucsd-haosulab\n",
    "spec:\n",
    "  ttlSecondsAfterFinished: 604800\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        nautilus.io/rl: \"true\"\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: gpu-container\n",
    "          image: albert01102/cuda12.4.1_ubuntu22.04_embodiment:isaac-v1.1-nodisplay\n",
    "          command:\n",
    "            - \"/bin/bash\"\n",
    "            - \"-c\"\n",
    "          args:\n",
    "            - |\n",
    "              cd /bai-fast-vol/code/embodiment-scaling-law && {command}\n",
    "          resources:\n",
    "            requests:\n",
    "              cpu: \"16\"\n",
    "              memory: \"160Gi\"\n",
    "              nvidia.com/gpu: \"1\"\n",
    "            limits:\n",
    "              cpu: \"32\"\n",
    "              memory: \"192Gi\"\n",
    "              nvidia.com/gpu: \"1\"\n",
    "          volumeMounts:\n",
    "            - name: dshm\n",
    "              mountPath: /dev/shm\n",
    "            - name: bai-fast-vol\n",
    "              mountPath: /bai-fast-vol\n",
    "      volumes:\n",
    "        - name: dshm\n",
    "          emptyDir:\n",
    "            medium: Memory\n",
    "        - name: bai-fast-vol\n",
    "          persistentVolumeClaim:\n",
    "            claimName: bai-fast-vol\n",
    "      restartPolicy: Never\n",
    "      affinity:\n",
    "        nodeAffinity:\n",
    "          requiredDuringSchedulingIgnoredDuringExecution:\n",
    "            nodeSelectorTerms:\n",
    "              - matchExpressions:\n",
    "                  - key: nvidia.com/gpu.product\n",
    "                    operator: In\n",
    "                    values:\n",
    "                      - NVIDIA-GeForce-RTX-4090\n",
    "                      - NVIDIA-GeForce-RTX-3090\n",
    "                      - NVIDIA-A100-80GB-PCIe-MIG-1g.10gb   # threaded multi-instance GPU\n",
    "                      - NVIDIA-A100-PCIE-40GB\n",
    "                      - NVIDIA-A100-80GB-PCIe\n",
    "                      - NVIDIA-A100-SXM4-80GB\n",
    "                      - NVIDIA-RTX-A6000  # 10% weaker than RTX 3090\n",
    "                      - NVIDIA-A40    # 20% weaker than RTX 3090\n",
    "  backoffLimit: 0\n",
    "\"\"\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Generate YAML files for each scaling factor\n",
    "for i, result in enumerate(detailed_results):  # Use detailed_results instead of scaled_train_test_sets\n",
    "    train_set = result[\"Train Set Names\"]\n",
    "    test_set = result[\"Test Set Names\"]\n",
    "    # print(train_set)\n",
    "    \n",
    "    # Use the long detailed folder names for train and test sets\n",
    "    train_set_str = \" \".join(train_set)\n",
    "    test_set_str = \" \".join(test_set)\n",
    "    \n",
    "    # Command to execute training for this scaling factor\n",
    "    command = (\n",
    "        f\"/workspace/isaaclab/isaaclab.sh -p scripts/rsl_rl/run_distillation.py \"\n",
    "        f\"--train_set {train_set_str} \"\n",
    "        f\"--test_set {test_set_str} \"\n",
    "        f\"--model urma \"\n",
    "        f\"--exp_name scaling_factor_{result['Scaling Factor']:.1f}_v3 \"\n",
    "        f\"--batch_size 256 \"\n",
    "        f\"--lr 3e-4 \"\n",
    "        f\"--num_workers 0 \"\n",
    "        f\"--max_files_in_memory 8 \"\n",
    "        f\"--num_epochs 10 \"\n",
    "        f\"--gradient_acc_steps 1 > ../logs_distillation/scaling_factor_{result['Scaling Factor']:.1f}_0107\"\n",
    "    )\n",
    "    \n",
    "    # Job name\n",
    "    job_name = f\"bai-distillation-scaling-{i+1}-v3\"\n",
    "    \n",
    "    # Generate YAML content\n",
    "    yaml_content = yaml_template.format(job_name=job_name, command=command)\n",
    "    \n",
    "    # Write to YAML file\n",
    "    yaml_file = os.path.join(output_folder, f\"{job_name}.yaml\")\n",
    "    with open(yaml_file, \"w\") as f:\n",
    "        f.write(yaml_content)\n",
    "\n",
    "print(f\"Generated {len(detailed_results)} job YAML files in '{output_folder}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2244a09-5ce1-4969-8231-9bac9d17a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission script: jobs_scaling_factors/submit_all_jobs.sh\n",
      "Deletion script: jobs_scaling_factors/delete_all_jobs.sh\n"
     ]
    }
   ],
   "source": [
    "# Paths for submission and deletion scripts\n",
    "submission_script = os.path.join(output_folder, \"submit_all_jobs.sh\")\n",
    "deletion_script = os.path.join(output_folder, \"delete_all_jobs.sh\")\n",
    "\n",
    "# Get all job files generated\n",
    "job_files = [f for f in os.listdir(output_folder) if f.endswith(\".yaml\")]\n",
    "\n",
    "# Generate submission script\n",
    "with open(submission_script, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\\n\")\n",
    "    for job_file in job_files:\n",
    "        f.write(f\"kubectl create -f {os.path.join(output_folder, job_file)}\\n\")\n",
    "\n",
    "# Make the submission script executable\n",
    "os.chmod(submission_script, 0o755)\n",
    "\n",
    "# Generate deletion script\n",
    "with open(deletion_script, \"w\") as f:\n",
    "    f.write(\"#!/bin/bash\\n\\n\")\n",
    "    for job_file in job_files:\n",
    "        job_name = job_file.replace(\".yaml\", \"\")  # Extract job name from the file name\n",
    "        f.write(f\"kubectl delete job {job_name}\\n\")\n",
    "\n",
    "# Make the deletion script executable\n",
    "os.chmod(deletion_script, 0o755)\n",
    "\n",
    "print(f\"Submission script: {submission_script}\")\n",
    "print(f\"Deletion script: {deletion_script}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ec6285-e0c5-4b1a-a53c-0b8b763019f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs_scaling_factors/\n",
      "jobs_scaling_factors/bai-distillation-scaling-3-v3.yaml\n",
      "jobs_scaling_factors/gen_distillation_commands.ipynb\n",
      "jobs_scaling_factors/bai-distillation-scaling-10-v3.yaml\n",
      "jobs_scaling_factors/submit_all_jobs.sh\n",
      "jobs_scaling_factors/bai-distillation-scaling-8-v3.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-5-v3.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-4-v3.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-7-v3.yaml\n",
      "jobs_scaling_factors/.ipynb_checkpoints/\n",
      "jobs_scaling_factors/.ipynb_checkpoints/bai-distillation-scaling-2-v3-checkpoint.yaml\n",
      "jobs_scaling_factors/.ipynb_checkpoints/gen_distillation_commands-checkpoint.ipynb\n",
      "jobs_scaling_factors/.ipynb_checkpoints/bai-distillation-scaling-10-v3-checkpoint.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-1-v3.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-2-v3.yaml\n",
      "jobs_scaling_factors/delete_all_jobs.sh\n",
      "jobs_scaling_factors/bai-distillation-scaling-9-v3.yaml\n",
      "jobs_scaling_factors/bai-distillation-scaling-6-v3.yaml\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf jobs_scaling_factors.tar jobs_scaling_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489057a5-7d85-45ba-815a-8ed9c3621c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
